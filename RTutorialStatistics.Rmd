---
title: "Statistics R: Tutorial"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library("shiny")
library("learnr")
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.cap = "Code", exercise.eval = FALSE, exercise.timelimit = NULL, exercise.lines = 3, exercise.checker = FALSE, exercise.completion = TRUE, exercise.diagnostics = TRUE, exercise.startover = TRUE)  
```





##TUTORIAL 5: Statistics with **R**

This is a brief introduction to common statistical tests and methods and how they can be carried out in **R**.

###
**R** is primarily a statistical tool and comes with a core set of statistical methods.  If *base* **R** does not have a function, it is likely that one of the thousands of contributed *packages* does.  

###
In this tutorial we will briefly look at:  

* Hypothesis testing:  
  * t-tests  
  * ???  
  * Anova  

###Hypothesis testing
We often encounter scenarios in ecology where we have to evaluate whether two or more sets of samples are different from each other.  e.g. 

* You are monitoring the stability of a colony of Greater Black-backed Gulls.  A contractor has counted the gulls five times a year for three years.  
* You are evaluating a grazing intervention on an upland grassland  You have measured the vegetation height in 20 random quadrats at the start of the intervention, and again after five years.

#### What is a sample? 
In each case you have *sampled* a *population*.  

Example | Samples | Populations 
:--- |:--- |:---
Gulls |number of gulls visible at each count |number of resident gulls in survey year
Grazing |mean vegetation height of the 20 quadrats at each survey |mean vegetation height in survey year 

###
It is easy to find out whether the **samples** are different.  But we actually want to know if the **populations** they come from are different.  

### The null hypothesis
What if the the actual mean vegetation height has not changed at all over time?  This is our **null hypothesis**.  Could we still have measured a different mean vegetation height when we repeated the survey?  

###
Yes, because:
* we may have unconsciously made our measurements differently (survey error); 
* we were drawn to areas of the parcel that had interesting plants in it (sampling bias); or 
* the vegetation in the 20 random quadrats we chose happened to be a different height (random error).  

###
Survey error and sampling bias are hard to detect.  But we can calculate the likelihood of the random error statistically by working out the chances of drawing our particular samples from a population at random.  This is called **hypothesis testing**.  But first we must make some assumptions.  

### Assumptions
Almost all statistical tests rely on assumptions about the data, and we must first determine whether they are met before we know which tests we can apply.  

###
Typical assumptions are:

* Normality: Data have a normal distribution, i.e. bell shaped
* Homogeneity of variances: Data from multiple groups have the same variance
* Linearity: Data have a linear relationship
* Independence: Data are independent


### Testing for normality

This is a typical normal distribution: 
 
```{r}
set.seed(43)
sample.data <- data.frame(measurement = rnorm(n = 2000, mean = 20, sd = 5), sample = as.factor(seq(1:2000)))
hist(sample.data$measurement, breaks = 50)
```

```{r}
qqplot(x = sample.data$measurement, y = sample.data$sample)
```


### Exercise: does it look normal

#### give three distributions adn ask person to select which doesn't look normal???


### Formal test for normality: 

The **Anderson Darling test** tests the assumption that the data is from a normal distribution.  If the p value is below 0.05 we assume that the distribution is not normal.  

```{r, eval=FALSE}
ad.test(dep.var)  #if p<0.05 then not normal
```

###
In many cases, transformation of the data will result ina  normal distribution.  In that case you can carry out your analysis with transformed data.  

```{r, eval=FALSE}
		hist(log(dep.var, base=10))
		boxplot(log(dep.var, base=10))
		ad.test(log(dep.var, base=10))
		
```


### Are variances homogeneous?  

Two useful tests for the homogeneity of variances are the **Bartlett test**, and the **Fligner-Killeen** test.

```{r, eval=FALSE}
# Bartlett Test of Homogeneity of Variances
bartlett.test(measure ~ group, data = df)
# if the p-value is < 0.05 then data not suitable for ANOVA

# The Fligner-Killeen test
with (df, fligner.test(measure, group))
# if the p-value is < 0.05 then data not suitable for ANOVA
```



